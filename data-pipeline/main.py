#!/usr/bin/env python3
"""
Jenosize Article Scraping and Fine-tuning Data Pipeline
Main orchestrator for the entire data collection and processing pipeline.
"""

import os
import sys
import time
import re
import argparse
from typing import List, Dict, Optional
from datetime import datetime

from config.settings import settings
from scrapers.category_scraper import CategoryScraper
from scrapers.article_scraper import ArticleScraper
from processors.content_extractor import ContentExtractor
from generators.dataset_builder import DatasetBuilder
from models.schemas import ProcessingStats, ArticleLink, ScrapingResult
from utils.logger import get_logger
from utils.helpers import save_json, load_json, ensure_dir_exists
from utils.csv_reader import CSVReader

logger = get_logger(__name__)

class DataPipeline:
    """Main data pipeline orchestrator"""
    
    def __init__(self, resume_from_step: Optional[str] = None):
        self.resume_from_step = resume_from_step
        self.stats = ProcessingStats()
        self.start_time = time.time()
        
        # Initialize components
        self.csv_reader = CSVReader()
        self.category_scraper = CategoryScraper()  # Keep for backward compatibility
        self.article_scraper = ArticleScraper(use_selenium=True)  # Enable Selenium
        self.content_extractor = ContentExtractor()
        self.dataset_builder = DatasetBuilder()
        
        # Ensure output directories exist
        ensure_dir_exists(settings.OUTPUT_DIR)
        ensure_dir_exists(settings.RAW_DIR)
        ensure_dir_exists(settings.PROCESSED_DIR)
    
    def run_pipeline(self, max_articles_per_category: Optional[int] = None) -> None:
        """Run the complete data pipeline"""
        
        logger.info("=" * 60)
        logger.info("üöÄ Starting Jenosize Article Scraping Pipeline")
        logger.info("=" * 60)
        
        try:
            # Step 1: Discover articles
            article_links = self._step_1_discover_articles()
            
            # Step 2: Scrape articles
            scraping_results = self._step_2_scrape_articles(article_links, max_articles_per_category)
            
            # Step 3: Process content
            processed_articles = self._step_3_process_content(scraping_results)
            
            # Step 4: Build datasets
            dataset_stats = self._step_4_build_datasets(processed_articles)
            
            # Step 5: Validate and finalize
            self._step_5_finalize(dataset_stats)
            
        except KeyboardInterrupt:
            logger.warning("‚èπÔ∏è  Pipeline interrupted by user")
            self._cleanup()
        except Exception as e:
            logger.error(f"üí• Pipeline failed with error: {e}")
            raise
        finally:
            self._cleanup()
    
    def _step_1_discover_articles(self) -> List[ArticleLink]:
        """Step 1: Load article links from CSV file"""
        
        step_name = "discover_articles"
        cache_file = os.path.join(settings.OUTPUT_DIR, "article_links.json")
        
        if self._should_skip_step(step_name, cache_file):
            logger.info("üìÇ Loading cached article links...")
            cached_data = load_json(cache_file)
            if cached_data:
                return [ArticleLink(**link_data) for link_data in cached_data['links']]
        
        logger.info("üîç Step 1: Loading articles from CSV file")
        
        try:
            # Read article links from CSV
            article_links = self.csv_reader.read_urls()
            
            if not article_links:
                logger.error("No articles found in CSV file")
                return []
            
            # Get category breakdown
            category_stats = self.csv_reader.get_category_stats(article_links)
            
            # Update statistics
            self.stats.total_categories = category_stats['total_categories']
            self.stats.total_links_found = len(article_links)
            self.stats.categories_breakdown = category_stats['by_category']
            
            # Save results
            links_data = {
                'discovery_time': datetime.now().isoformat(),
                'total_links': len(article_links),
                'category_stats': category_stats,
                'links': [
                    {
                        'url': link.url,
                        'title': link.title,
                        'category': link.category,
                        'href': link.href
                    }
                    for link in article_links
                ]
            }
            
            save_json(links_data, cache_file)
            
            logger.info(f"‚úÖ Loaded {len(article_links)} articles from CSV across {len(category_stats['by_category'])} categories")
            
            # Log category breakdown
            for category, count in category_stats['by_category'].items():
                logger.info(f"   üìë {category}: {count} articles")
            
            return article_links
            
        except Exception as e:
            logger.error(f"‚ùå Step 1 failed: {e}")
            raise
    
    def _step_2_scrape_articles(self, 
                               article_links: List[ArticleLink], 
                               max_articles_per_category: Optional[int] = None) -> List[ScrapingResult]:
        """Step 2: Scrape article content"""
        
        step_name = "scrape_articles"
        cache_file = os.path.join(settings.OUTPUT_DIR, "scraping_results.json")
        
        if self._should_skip_step(step_name, cache_file):
            logger.info("üìÇ Loading cached scraping results...")
            cached_data = load_json(cache_file)
            if cached_data and cached_data.get('results'):
                logger.info(f"Loading {len(cached_data['results'])} cached scraping results...")
                # Skip loading from cache for now - it's complex to reconstruct full ScrapingResult objects
                # Let the pipeline proceed to load from processed_articles.json instead
                pass
        
        logger.info(f"üì∞ Step 2: Scraping {len(article_links)} articles")
        
        # Limit articles if specified
        if max_articles_per_category:
            original_count = len(article_links)
            article_links = self._limit_articles_per_category(article_links, max_articles_per_category)
            logger.info(f"   üìä Limited to {len(article_links)}/{original_count} articles ({max_articles_per_category} per category)")
        
        try:
            # Scrape articles
            scraping_results = self.article_scraper.scrape_articles(article_links)
            
            # Update statistics
            self.stats.total_articles_scraped = len(scraping_results)
            self.stats.successful_scrapes = sum(1 for r in scraping_results if r.success)
            self.stats.failed_scrapes = self.stats.total_articles_scraped - self.stats.successful_scrapes
            
            # Calculate content statistics
            total_content_chars = 0
            for result in scraping_results:
                if result.success and result.content:
                    content_length = len(result.content.text_content)
                    total_content_chars += content_length
            
            self.stats.total_content_chars = total_content_chars
            
            # Save results (simplified for caching)
            results_data = {
                'scraping_time': datetime.now().isoformat(),
                'total_scraped': len(scraping_results),
                'successful': self.stats.successful_scrapes,
                'failed': self.stats.failed_scrapes,
                'results': [
                    {
                        'success': result.success,
                        'url': result.url,
                        'error': result.error,
                        'processing_time': result.processing_time,
                        'has_content': result.content is not None
                    }
                    for result in scraping_results
                ]
            }
            
            save_json(results_data, cache_file)
            
            logger.info(f"‚úÖ Scraping completed: {self.stats.successful_scrapes}/{len(scraping_results)} successful")
            logger.info(f"   üìä Success rate: {self.stats.success_rate:.1f}%")
            logger.info(f"   üìù Total content: {total_content_chars:,} characters")
            
            return scraping_results
            
        except Exception as e:
            logger.error(f"‚ùå Step 2 failed: {e}")
            raise
    
    def _step_3_process_content(self, scraping_results: List[ScrapingResult]) -> List[Dict]:
        """Step 3: Process scraped content for training"""
        
        step_name = "process_content"
        cache_file = os.path.join(settings.OUTPUT_DIR, "processed_articles.json")
        
        # Check if we can load from cache
        if self._should_skip_step(step_name, cache_file):
            logger.info("üìÇ Loading cached processed articles...")
            cached_data = load_json(cache_file)
            if cached_data and cached_data.get('articles'):
                logger.info(f"Loaded {len(cached_data['articles'])} processed articles from cache")
                return cached_data['articles']
        
        logger.info("‚öôÔ∏è  Step 3: Processing content for training data")
        
        processed_articles = []
        
        # Filter successful results
        successful_results = [r for r in scraping_results if r.success and r.content]
        
        logger.info(f"   üìä Processing {len(successful_results)} successfully scraped articles")
        
        for result in successful_results:
            try:
                if not result.content:
                    continue
                
                # Convert to dict for processing
                content_dict = {
                    'metadata': {
                        'url': result.content.metadata.url,
                        'title': result.content.metadata.title,
                        'category': result.content.metadata.category,
                        'author': result.content.metadata.author,
                        'publish_date': result.content.metadata.publish_date,
                        'description': result.content.metadata.description,
                        'tags': result.content.metadata.tags,
                        'content_stats': result.content.metadata.content_stats
                    },
                    'html_content': result.content.html_content,
                    'text_content': result.content.text_content,
                    'structured_content': result.content.structured_content,
                    'images': result.content.images
                }
                
                # NEW: Process content with HTML to Markdown conversion
                logger.info(f"Processing article: {result.content.metadata.title}")
                processed_content = self.content_extractor.extract_training_content(content_dict)
                
                if processed_content:
                    processed_articles.append(processed_content)
                    logger.info(f"‚úÖ Successfully processed: {result.content.metadata.title}")
                else:
                    logger.warning(f"‚ö†Ô∏è  Skipped article (quality check failed): {result.content.metadata.title}")
                    continue
                
            except Exception as e:
                logger.warning(f"Error processing article {result.url}: {e}")
                continue
        
        # Save processed articles with markdown content
        if processed_articles:
            processed_file = os.path.join(settings.OUTPUT_DIR, "processed_articles.json")
            processed_data = {
                'processing_time': datetime.now().isoformat(),
                'total_processed': len(processed_articles),
                'articles': processed_articles
            }
            save_json(processed_data, processed_file)
            logger.info(f"üíæ Saved processed articles to {processed_file}")
            
            # Save markdown files separately for inspection
            markdown_dir = os.path.join(settings.OUTPUT_DIR, "markdown")
            ensure_dir_exists(markdown_dir)
            
            for i, article in enumerate(processed_articles):
                if article.get('markdown_content'):
                    title = article['metadata'].get('title', f'article_{i}')
                    # Clean title for filename
                    safe_title = re.sub(r'[^\w\s-]', '', title).strip()
                    safe_title = re.sub(r'[-\s]+', '-', safe_title)[:50]
                    
                    markdown_file = os.path.join(markdown_dir, f"{safe_title}.md")
                    try:
                        with open(markdown_file, 'w', encoding='utf-8') as f:
                            f.write(article['markdown_content'])
                    except Exception as e:
                        logger.warning(f"Could not save markdown file {markdown_file}: {e}")
            
            logger.info(f"üìù Saved {len(processed_articles)} markdown files to {markdown_dir}")
        
        logger.info(f"‚úÖ Content processing completed: {len(processed_articles)} articles ready for training")
        
        return processed_articles
    
    def _step_4_build_datasets(self, processed_articles: List[Dict]) -> Dict:
        """Step 4: Build training and validation datasets"""
        
        logger.info("üèóÔ∏è  Step 4: Building training datasets")
        
        try:
            # Build datasets
            dataset_stats = self.dataset_builder.build_datasets(processed_articles)
            
            logger.info(f"‚úÖ Dataset building completed:")
            logger.info(f"   üìö Training examples: {dataset_stats.train_examples}")
            logger.info(f"   üîç Validation examples: {dataset_stats.validation_examples}")
            logger.info(f"   üéØ Categories covered: {len(dataset_stats.categories_covered)}")
            logger.info(f"   üìè Avg system prompt: {dataset_stats.avg_system_prompt_length} chars")
            logger.info(f"   üìè Avg user input: {dataset_stats.avg_user_input_length} chars")
            logger.info(f"   üìè Avg assistant response: {dataset_stats.avg_assistant_response_length} chars")
            
            return dataset_stats
            
        except Exception as e:
            logger.error(f"‚ùå Step 4 failed: {e}")
            raise
    
    def _step_5_finalize(self, dataset_stats: Dict) -> None:
        """Step 5: Validate datasets and create final report"""
        
        logger.info("üéØ Step 5: Finalizing and validating datasets")
        
        try:
            # Validate datasets
            train_validation = self.dataset_builder.validate_dataset(
                os.path.join(settings.OUTPUT_DIR, 'train.jsonl')
            )
            
            val_validation = self.dataset_builder.validate_dataset(
                os.path.join(settings.OUTPUT_DIR, 'validation.jsonl')
            )
            
            # Calculate final statistics
            self.stats.processing_time_seconds = time.time() - self.start_time
            
            # Create final report
            self._create_final_report(dataset_stats, train_validation, val_validation)
            
            logger.info("‚úÖ Pipeline completed successfully!")
            logger.info(f"   ‚è±Ô∏è  Total processing time: {self.stats.processing_time_seconds:.1f} seconds")
            logger.info(f"   üìä Success rate: {self.stats.success_rate:.1f}%")
            
        except Exception as e:
            logger.error(f"‚ùå Step 5 failed: {e}")
            raise
    
    def _create_final_report(self, dataset_stats: Dict, train_validation: Dict, val_validation: Dict) -> None:
        """Create final pipeline report"""
        
        report = {
            'pipeline_summary': {
                'execution_time': datetime.now().isoformat(),
                'processing_time_seconds': self.stats.processing_time_seconds,
                'total_categories_scraped': self.stats.total_categories,
                'total_links_discovered': self.stats.total_links_found,
                'total_articles_scraped': self.stats.total_articles_scraped,
                'successful_scrapes': self.stats.successful_scrapes,
                'failed_scrapes': self.stats.failed_scrapes,
                'success_rate_percent': self.stats.success_rate,
                'total_content_characters': self.stats.total_content_chars,
                'categories_breakdown': self.stats.categories_breakdown
            },
            'dataset_stats': dataset_stats.__dict__ if hasattr(dataset_stats, '__dict__') else dataset_stats,
            'validation_results': {
                'train_dataset': train_validation,
                'validation_dataset': val_validation
            },
            'output_files': {
                'train_dataset': os.path.join(settings.OUTPUT_DIR, 'train.jsonl'),
                'validation_dataset': os.path.join(settings.OUTPUT_DIR, 'validation.jsonl'),
                'dataset_metadata': os.path.join(settings.OUTPUT_DIR, 'dataset_metadata.json'),
                'raw_html_files': settings.RAW_DIR,
                'processed_content': settings.PROCESSED_DIR
            }
        }
        
        # Save report
        report_path = os.path.join(settings.OUTPUT_DIR, 'pipeline_report.json')
        save_json(report, report_path)
        
        logger.info(f"üìã Final report saved to: {report_path}")
    
    def _should_skip_step(self, step_name: str, cache_file: str) -> bool:
        """Check if step should be skipped due to resume logic"""
        if not self.resume_from_step:
            return False
        
        step_order = ['discover_articles', 'scrape_articles', 'process_content', 'build_datasets', 'finalize']
        
        if step_name not in step_order:
            return False
        
        current_step_index = step_order.index(step_name)
        resume_step_index = step_order.index(self.resume_from_step) if self.resume_from_step in step_order else -1
        
        # Skip if current step is before resume step and cache exists
        return current_step_index < resume_step_index and os.path.exists(cache_file)
    
    def _limit_articles_per_category(self, 
                                   article_links: List[ArticleLink], 
                                   max_per_category: int) -> List[ArticleLink]:
        """Limit number of articles per category"""
        
        category_counts = {}
        limited_links = []
        
        for link in article_links:
            category = link.category
            if category not in category_counts:
                category_counts[category] = 0
            
            if category_counts[category] < max_per_category:
                limited_links.append(link)
                category_counts[category] += 1
        
        return limited_links
    
    def _cleanup(self) -> None:
        """Cleanup resources"""
        try:
            self.category_scraper.close()
            self.article_scraper.close()
        except Exception as e:
            logger.warning(f"Error during cleanup: {e}")

def main():
    """Main entry point"""
    
    parser = argparse.ArgumentParser(
        description="Jenosize Article Scraping and Fine-tuning Data Pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python main.py                           # Run full pipeline
  python main.py --limit 10                # Limit to 10 articles per category
  python main.py --resume process_content  # Resume from content processing
  python main.py --limit 5 --resume scrape_articles  # Resume with limits
        """
    )
    
    parser.add_argument(
        '--limit', 
        type=int, 
        help='Maximum articles per category (useful for development/testing)'
    )
    
    parser.add_argument(
        '--resume', 
        choices=['discover_articles', 'scrape_articles', 'process_content', 'build_datasets', 'finalize'],
        help='Resume pipeline from specific step (uses cached data where available)'
    )
    
    parser.add_argument(
        '--output-dir',
        help='Custom output directory (overrides config)'
    )
    
    args = parser.parse_args()
    
    # Override output directory if specified
    if args.output_dir:
        settings.OUTPUT_DIR = os.path.abspath(args.output_dir)
        settings.RAW_DIR = os.path.join(settings.OUTPUT_DIR, "raw")
        settings.PROCESSED_DIR = os.path.join(settings.OUTPUT_DIR, "processed")
        logger.info(f"Using custom output directory: {settings.OUTPUT_DIR}")
    
    try:
        # Initialize and run pipeline
        pipeline = DataPipeline(resume_from_step=args.resume)
        pipeline.run_pipeline(max_articles_per_category=args.limit)
        
    except KeyboardInterrupt:
        logger.info("üëã Pipeline interrupted. Goodbye!")
        sys.exit(1)
    except Exception as e:
        logger.error(f"üí• Pipeline failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()